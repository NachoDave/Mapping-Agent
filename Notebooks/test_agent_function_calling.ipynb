{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0dcee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import ollama\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc9b7ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 Define Python Tool (Function)\n",
    "def get_current_weather(location: str, unit: str = \"celcius\") -> str:\n",
    "    \"\"\"\n",
    "    Gets the current weather for a specified location.\n",
    "    Args:\n",
    "        location (str): The city or region to get weather for.\n",
    "        unit (str): The unit of temperature, either 'celsius' or 'fahrenheit'.\n",
    "    Returns:\n",
    "        str: A description of the weather.\n",
    "    \"\"\"\n",
    "    # In a real application you would make an API call\n",
    "    if location.lower() == \"manchester\":\n",
    "        if unit.lower() == \"fahrenheit\":\n",
    "            return \"Currently 59°F and partly cloudy in Manchester.\"\n",
    "        else:\n",
    "            return \"Currently 15°C and partly cloudy in Manchester.\"\n",
    "    elif location.lower() == \"london\":\n",
    "        return f\"Currently 20°C and sunny in London. Unit: {unit}.\"\n",
    "    else:\n",
    "        return f\"Sorry, I don't have weather data for {location}.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "badff3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 Describe your tool to the LLM (JSON Schema)\n",
    "# This is how you tell the LLM about the function's name, purpose, and parameters.\n",
    "tools_schema = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\", # Must match the Python function name exactly\n",
    "        \"description\": \"Gets the current weather for a specified location.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city or region to get weather for. E.g., 'Manchester', 'New York'.\"\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The unit of temperature, either 'celsius' or 'fahrenheit'. Defaults to 'celsius'.\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"] # Optional: provide allowed values\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"] # 'location' is a mandatory parameter\n",
    "        }\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e90b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Set up LLM interaction\n",
    "def chat_with_llm_with_tools(user_query: str, model_name: str = \"qwen3:8b\"):\n",
    "    \"\"\"\n",
    "    Sends a query to the LLM, including tool definitions, and processes the response.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "            You are a helpful AI assistant with access to tools.\n",
    "            You should use the 'get_current_weather' tool to answer questions about the weather.\n",
    "            Always prefer calling a tool if the user's intent matches a tool's function.\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_query\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n--- User Query: '{user_query}' ---\") # print out the user query\n",
    "    print(f\"--- Sending to Ollama model: '{model_name}' with defined tools ---\")\n",
    "    \n",
    "    try:\n",
    "        # give input to the model\n",
    "        response = ollama.chat( \n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            tools=tools_schema,\n",
    "            options={\n",
    "                \"temperature\": 0.0 \n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # 4. Process the LLMs Response\n",
    "        # Check \n",
    "        if 'tool_calls' in response['message'] and response['message']['tool_calls']:\n",
    "            print(\"\\n--- LLM decided to call a tool! ---\")\n",
    "            for tool_call in response['message']['tool_calls']:\n",
    "                tool_name = tool_call['function']['name']\n",
    "                tool_args = tool_call['function']['arguments']\n",
    "                tool_id = tool_call['id'] # Each tool call has a unique ID\n",
    "\n",
    "                print(f\"  Tool Name: {tool_name}\")\n",
    "                print(f\"  Tool Arguments: {json.dumps(tool_args, indent=2)}\")\n",
    "\n",
    "                # Here, you would execute the actual Python function based on tool_name\n",
    "                if tool_name == \"get_current_weather\":\n",
    "                    # Call your Python function with the arguments extracted by the LLM\n",
    "                    weather_result = get_current_weather(**tool_args)\n",
    "                    print(f\"\\n--- Executing tool '{tool_name}'... ---\")\n",
    "                    print(f\"  Tool Execution Result: {weather_result}\")\n",
    "                    \n",
    "        else:\n",
    "            # LLM returned a direct text response (no tool call)\n",
    "            print(\"\\n--- LLM returned a direct text response (no tool call): ---\")\n",
    "            print(response['message']['content'])\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred {e}\")\n",
    "\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bc940a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- User Query: 'What's the weather like in Manchester?' ---\n",
      "--- Sending to Ollama model: 'qwen3:8b' with defined tools ---\n",
      "\n",
      "--- LLM decided to call a tool! ---\n",
      "An error occurred 'id'\n",
      "\n",
      "--- User Query: 'Tell me a fun fact about giraffes.' ---\n",
      "--- Sending to Ollama model: 'qwen3:8b' with defined tools ---\n",
      "\n",
      "--- LLM returned a direct text response (no tool call): ---\n",
      "<think>\n",
      "Okay, the user asked for a fun fact about giraffes. Let me think. I need to check if any of the provided tools can help with that. The tools section is empty, so there's no function to call. Since the user's request is just for information and not related to weather or any other tool functions, I should answer directly. I'll share a fun fact about giraffes without using any tools.\n",
      "</think>\n",
      "\n",
      "Here's a fun fact about giraffes: Did you know that a giraffe's neck is actually about the same length as an elephant's neck? The misconception that giraffes have long necks to reach leaves high up in trees is a classic example of evolutionary adaptation, but their necks are more adapted for browsing and social interactions rather than just reaching for food.\n"
     ]
    }
   ],
   "source": [
    "# Run examples\n",
    "r = chat_with_llm_with_tools(user_query = \"What's the weather like in Manchester?\")\n",
    "t = chat_with_llm_with_tools(user_query = \"Tell me a fun fact about giraffes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec00ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "r['message']['tool_calls'], t['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bb0def8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- User Query: 'How many degrees Fahrenheit is it in London?' ---\n",
      "--- Sending to Ollama model: 'qwen3:8b' with defined tools ---\n",
      "\n",
      "--- LLM decided to call a tool! ---\n",
      "  Tool Name: get_current_weather\n",
      "  Tool Arguments: {\n",
      "  \"location\": \"London\",\n",
      "  \"unit\": \"Fahrenheit\"\n",
      "}\n",
      "\n",
      "--- Executing tool 'get_current_weather'... ---\n",
      "  Tool Execution Result: Currently 20°C and sunny in London. Unit: Fahrenheit.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import json\n",
    "\n",
    "# --- 1. Define your Python Tool (Function) ---\n",
    "# This is the actual function that performs the desired action.\n",
    "def get_current_weather(location: str, unit: str = \"celsius\") -> str:\n",
    "    \"\"\"\n",
    "    Gets the current weather for a specified location.\n",
    "    Args:\n",
    "        location (str): The city or region to get weather for.\n",
    "        unit (str): The unit of temperature, either 'celsius' or 'fahrenheit'.\n",
    "    Returns:\n",
    "        str: A description of the weather.\n",
    "    \"\"\"\n",
    "    # In a real application, you would make an API call here (e.g., to OpenWeatherMap)\n",
    "    # For this basic example, we'll return a hardcoded response.\n",
    "    if location.lower() == \"manchester\":\n",
    "        if unit.lower() == \"fahrenheit\":\n",
    "            return \"Currently 59°F and partly cloudy in Manchester.\"\n",
    "        else:\n",
    "            return \"Currently 15°C and partly cloudy in Manchester.\"\n",
    "    elif location.lower() == \"london\":\n",
    "        return f\"Currently 20°C and sunny in London. Unit: {unit}.\"\n",
    "    else:\n",
    "        return f\"Sorry, I don't have weather data for {location}.\"\n",
    "\n",
    "# --- 2. Describe your Tool to the LLM (JSON Schema) ---\n",
    "# This is how you tell the LLM about the function's name, purpose, and parameters.\n",
    "tools_schema = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\", # Must match the Python function name exactly\n",
    "        \"description\": \"Gets the current weather for a specified location.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city or region to get weather for. E.g., 'Manchester', 'New York'.\"\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The unit of temperature, either 'celsius' or 'fahrenheit'. Defaults to 'celsius'.\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"] # Optional: provide allowed values\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"] # 'location' is a mandatory parameter\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# --- 3. Set up the LLM Interaction ---\n",
    "def chat_with_llm_with_tools(user_query: str, model_name: str = \"qwen3:8b\"):\n",
    "    \"\"\"\n",
    "    Sends a query to the LLM, including tool definitions, and processes the response.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "            You are a helpful AI assistant with access to tools.\n",
    "            You should use the 'get_current_weather' tool to answer questions about the weather.\n",
    "            Always prefer calling a tool if the user's intent matches a tool's function.\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_query\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n--- User Query: '{user_query}' ---\")\n",
    "    print(f\"--- Sending to Ollama model: '{model_name}' with defined tools ---\")\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            tools=tools_schema, # This is where you pass the tool definitions\n",
    "            options={\n",
    "                \"temperature\": 0.0 # Make it more deterministic for tool calling demos\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # --- 4. Process the LLM's Response ---\n",
    "        # Check if the LLM decided to call a tool\n",
    "        if 'tool_calls' in response['message'] and response['message']['tool_calls']:\n",
    "            print(\"\\n--- LLM decided to call a tool! ---\")\n",
    "            for tool_call in response['message']['tool_calls']:\n",
    "                tool_name = tool_call['function']['name']\n",
    "                tool_args = tool_call['function']['arguments']\n",
    "                #tool_id = tool_call['id'] # Each tool call has a unique ID\n",
    "\n",
    "                print(f\"  Tool Name: {tool_name}\")\n",
    "                print(f\"  Tool Arguments: {json.dumps(tool_args, indent=2)}\")\n",
    "\n",
    "                # Here, you would execute the actual Python function based on tool_name\n",
    "                if tool_name == \"get_current_weather\":\n",
    "                    # Call your Python function with the arguments extracted by the LLM\n",
    "                    weather_result = get_current_weather(**tool_args)\n",
    "                    print(f\"\\n--- Executing tool '{tool_name}'... ---\")\n",
    "                    print(f\"  Tool Execution Result: {weather_result}\")\n",
    "\n",
    "                    # Optionally: Send the tool's output back to the LLM for it to summarize or refine\n",
    "                    # This is how multi-turn reasoning with tools works.\n",
    "                    # messages.append({\"role\": \"tool\", \"content\": weather_result, \"tool_call_id\": tool_id})\n",
    "                    # final_response = ollama.chat(model=model_name, messages=messages)\n",
    "                    # print(f\"\\n--- LLM's final response after tool execution: ---\")\n",
    "                    # print(final_response['message']['content'])\n",
    "\n",
    "                else:\n",
    "                    print(f\"Error: Unknown tool '{tool_name}' requested by LLM.\")\n",
    "        else:\n",
    "            # LLM returned a direct text response (no tool call)\n",
    "            print(\"\\n--- LLM returned a direct text response (no tool call): ---\")\n",
    "            print(response['message']['content'])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(\"Please ensure your Ollama server is running and the model is downloaded (`ollama run llama3.2`).\")\n",
    "\n",
    "# --- Run the Example Queries ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Test case 1: User asks a question that requires the tool\n",
    "    #chat_with_llm_with_tools(\"What's the weather like in Manchester?\", model_name=\"qwen3:8b\")\n",
    "\n",
    "    # Test case 2: User asks with a specific unit\n",
    "    chat_with_llm_with_tools(\"How many degrees Fahrenheit is it in London?\", model_name=\"qwen3:8b\")\n",
    "\n",
    "    # Test case 3: User asks about a location not handled by our dummy tool\n",
    "    #chat_with_llm_with_tools(\"What's the temperature in Paris?\", model_name=\"qwen3:8b\")\n",
    "\n",
    "    # Test case 4: User asks a general question (should not call tool)\n",
    "    #chat_with_llm_with_tools(\"Tell me a fun fact about giraffes.\", model_name=\"qwen3:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d75f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Very simple tool calling\n",
    "def add_two_numbers(a: int, b: int) -> int:\n",
    "  \"\"\"\n",
    "  Add two numbers\n",
    "\n",
    "  Args:\n",
    "    a: The first integer number\n",
    "    b: The second integer number\n",
    "\n",
    "  Returns:\n",
    "    int: The sum of the two numbers\n",
    "  \"\"\"\n",
    "  return a + b\n",
    "\n",
    "def subtract_two_numbers(a: int, b: int) -> int:\n",
    "  \"\"\"\n",
    "  Add two numbers\n",
    "\n",
    "  Args:\n",
    "    a: The first integer number\n",
    "    b: The second integer number\n",
    "\n",
    "  Returns:\n",
    "    int: Subtraction of one number from another\n",
    "  \"\"\"\n",
    "  return a - b\n",
    "\n",
    "\n",
    "response = ollama.chat(\n",
    "  'qwen3:8b',\n",
    "  messages=[{'role': 'user', 'content': 'What is 10 - 10?'}],\n",
    "  tools=[add_two_numbers, subtract_two_numbers], # Actual function reference\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f77af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_sub = ollama.chat(\n",
    "  'qwen3:8b',\n",
    "  messages=[{'role': 'user', 'content': 'Select a suitable tool for the problem. If no suitable tool available then tell a joke'}, {'role': 'user', 'content': 'What is 10 - 10?'}],\n",
    "  tools=[add_two_numbers, subtract_two_numbers], # Actual function reference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14166847",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_joke = ollama.chat(\n",
    "  'qwen3:8b',\n",
    "  messages=[{'role': 'user', 'content': 'Select a suitable tool for the problem. If no suitable tool available then tell a joke'}, {'role': 'user', 'content': 'What is the captial of India?'}],\n",
    "  tools=[add_two_numbers, subtract_two_numbers], # Actual function reference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e701d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([ToolCall(function=Function(name='add_two_numbers', arguments={'a': 10, 'b': 10}))],\n",
       " [ToolCall(function=Function(name='subtract_two_numbers', arguments={'a': 10, 'b': 10}))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.message.tool_calls, response_sub.message.tool_calls\n",
    "#print(response_joke.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb1ddc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
