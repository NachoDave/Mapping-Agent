{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a0dcee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import ollama\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc9b7ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1 Define Python Tool (Function)\n",
    "def get_current_weather(location: str, unit: str = \"celcius\") -> str:\n",
    "    \"\"\"\n",
    "    Gets the current weather for a specified location.\n",
    "    Args:\n",
    "        location (str): The city or region to get weather for.\n",
    "        unit (str): The unit of temperature, either 'celsius' or 'fahrenheit'.\n",
    "    Returns:\n",
    "        str: A description of the weather.\n",
    "    \"\"\"\n",
    "    # In a real application you would make an API call\n",
    "    if location.lower() == \"manchester\":\n",
    "        if unit.lower() == \"fahrenheit\":\n",
    "            return \"Currently 59°F and partly cloudy in Manchester.\"\n",
    "        else:\n",
    "            return \"Currently 15°C and partly cloudy in Manchester.\"\n",
    "    elif location.lower() == \"london\":\n",
    "        return f\"Currently 20°C and sunny in London. Unit: {unit}.\"\n",
    "    else:\n",
    "        return f\"Sorry, I don't have weather data for {location}.\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "badff3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 Describe your tool to the LLM (JSON Schema)\n",
    "# This is how you tell the LLM about the function's name, purpose, and parameters.\n",
    "tools_schema = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\", # Must match the Python function name exactly\n",
    "        \"description\": \"Gets the current weather for a specified location.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city or region to get weather for. E.g., 'Manchester', 'New York'.\"\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The unit of temperature, either 'celsius' or 'fahrenheit'. Defaults to 'celsius'.\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"] # Optional: provide allowed values\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"] # 'location' is a mandatory parameter\n",
    "        }\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e90b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Set up LLM interaction\n",
    "def chat_with_llm_with_tools(user_query: str, model_name: str = \"qwen3:8b\"):\n",
    "    \"\"\"\n",
    "    Sends a query to the LLM, including tool definitions, and processes the response.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "            You are a helpful AI assistant with access to tools.\n",
    "            You should use the 'get_current_weather' tool to answer questions about the weather.\n",
    "            Always prefer calling a tool if the user's intent matches a tool's function.\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_query\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n--- User Query: '{user_query}' ---\") # print out the user query\n",
    "    print(f\"--- Sending to Ollama model: '{model_name}' with defined tools ---\")\n",
    "    \n",
    "    try:\n",
    "        # give input to the model\n",
    "        response = ollama.chat( \n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            tools=tools_schema,\n",
    "            options={\n",
    "                \"temperature\": 0.0 \n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # 4. Process the LLMs Response\n",
    "        # Check \n",
    "        if 'tool_calls' in response['message'] and response['message']['tool_calls']:\n",
    "            print(\"\\n--- LLM decided to call a tool! ---\")\n",
    "            for tool_call in response['message']['tool_calls']:\n",
    "                tool_name = tool_call['function']['name']\n",
    "                tool_args = tool_call['function']['arguments']\n",
    "                tool_id = tool_call['id'] # Each tool call has a unique ID\n",
    "\n",
    "                print(f\"  Tool Name: {tool_name}\")\n",
    "                print(f\"  Tool Arguments: {json.dumps(tool_args, indent=2)}\")\n",
    "\n",
    "                # Here, you would execute the actual Python function based on tool_name\n",
    "                if tool_name == \"get_current_weather\":\n",
    "                    # Call your Python function with the arguments extracted by the LLM\n",
    "                    weather_result = get_current_weather(**tool_args)\n",
    "                    print(f\"\\n--- Executing tool '{tool_name}'... ---\")\n",
    "                    print(f\"  Tool Execution Result: {weather_result}\")\n",
    "                    \n",
    "        else:\n",
    "            # LLM returned a direct text response (no tool call)\n",
    "            print(\"\\n--- LLM returned a direct text response (no tool call): ---\")\n",
    "            print(response['message']['content'])\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred {e}\")\n",
    "\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bc940a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- User Query: 'What's the weather like in Manchester?' ---\n",
      "--- Sending to Ollama model: 'qwen3:8b' with defined tools ---\n",
      "An error occurred model requires more system memory (6.6 GiB) than is available (5.6 GiB) (status code: 500)\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'response' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run examples\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m r = \u001b[43mchat_with_llm_with_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms the weather like in Manchester?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m t = chat_with_llm_with_tools(user_query = \u001b[33m\"\u001b[39m\u001b[33mTell me a fun fact about giraffes.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mchat_with_llm_with_tools\u001b[39m\u001b[34m(user_query, model_name)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAn error occurred \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse\u001b[49m\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'response' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# Run examples\n",
    "r = chat_with_llm_with_tools(user_query = \"What's the weather like in Manchester?\")\n",
    "t = chat_with_llm_with_tools(user_query = \"Tell me a fun fact about giraffes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec00ff77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([ToolCall(function=Function(name='get_current_weather', arguments={'city': 'Manchester'}))],\n",
       " Message(role='assistant', content='', images=None, tool_calls=[ToolCall(function=Function(name='get_current_weather', arguments={}))]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r['message']['tool_calls'], t['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb0def8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- User Query: 'What's the weather like in Manchester?' ---\n",
      "--- Sending to Ollama model: 'llama3' with defined tools ---\n",
      "An error occurred: model \"llama3\" not found, try pulling it first (status code: 404)\n",
      "Please ensure your Ollama server is running and the model is downloaded (`ollama run llama3.2`).\n",
      "\n",
      "--- User Query: 'How many degrees Fahrenheit is it in London?' ---\n",
      "--- Sending to Ollama model: 'llama3' with defined tools ---\n",
      "An error occurred: model \"llama3\" not found, try pulling it first (status code: 404)\n",
      "Please ensure your Ollama server is running and the model is downloaded (`ollama run llama3.2`).\n",
      "\n",
      "--- User Query: 'What's the temperature in Paris?' ---\n",
      "--- Sending to Ollama model: 'llama3' with defined tools ---\n",
      "An error occurred: model \"llama3\" not found, try pulling it first (status code: 404)\n",
      "Please ensure your Ollama server is running and the model is downloaded (`ollama run llama3.2`).\n",
      "\n",
      "--- User Query: 'Tell me a fun fact about giraffes.' ---\n",
      "--- Sending to Ollama model: 'llama3' with defined tools ---\n",
      "An error occurred: model \"llama3\" not found, try pulling it first (status code: 404)\n",
      "Please ensure your Ollama server is running and the model is downloaded (`ollama run llama3.2`).\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import json\n",
    "\n",
    "# --- 1. Define your Python Tool (Function) ---\n",
    "# This is the actual function that performs the desired action.\n",
    "def get_current_weather(location: str, unit: str = \"celsius\") -> str:\n",
    "    \"\"\"\n",
    "    Gets the current weather for a specified location.\n",
    "    Args:\n",
    "        location (str): The city or region to get weather for.\n",
    "        unit (str): The unit of temperature, either 'celsius' or 'fahrenheit'.\n",
    "    Returns:\n",
    "        str: A description of the weather.\n",
    "    \"\"\"\n",
    "    # In a real application, you would make an API call here (e.g., to OpenWeatherMap)\n",
    "    # For this basic example, we'll return a hardcoded response.\n",
    "    if location.lower() == \"manchester\":\n",
    "        if unit.lower() == \"fahrenheit\":\n",
    "            return \"Currently 59°F and partly cloudy in Manchester.\"\n",
    "        else:\n",
    "            return \"Currently 15°C and partly cloudy in Manchester.\"\n",
    "    elif location.lower() == \"london\":\n",
    "        return f\"Currently 20°C and sunny in London. Unit: {unit}.\"\n",
    "    else:\n",
    "        return f\"Sorry, I don't have weather data for {location}.\"\n",
    "\n",
    "# --- 2. Describe your Tool to the LLM (JSON Schema) ---\n",
    "# This is how you tell the LLM about the function's name, purpose, and parameters.\n",
    "tools_schema = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\", # Must match the Python function name exactly\n",
    "        \"description\": \"Gets the current weather for a specified location.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city or region to get weather for. E.g., 'Manchester', 'New York'.\"\n",
    "                },\n",
    "                \"unit\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The unit of temperature, either 'celsius' or 'fahrenheit'. Defaults to 'celsius'.\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"] # Optional: provide allowed values\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"] # 'location' is a mandatory parameter\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# --- 3. Set up the LLM Interaction ---\n",
    "def chat_with_llm_with_tools(user_query: str, model_name: str = \"llama3.2\"):\n",
    "    \"\"\"\n",
    "    Sends a query to the LLM, including tool definitions, and processes the response.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "            You are a helpful AI assistant with access to tools.\n",
    "            You should use the 'get_current_weather' tool to answer questions about the weather.\n",
    "            Always prefer calling a tool if the user's intent matches a tool's function.\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_query\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n--- User Query: '{user_query}' ---\")\n",
    "    print(f\"--- Sending to Ollama model: '{model_name}' with defined tools ---\")\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            tools=tools_schema, # This is where you pass the tool definitions\n",
    "            options={\n",
    "                \"temperature\": 0.0 # Make it more deterministic for tool calling demos\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # --- 4. Process the LLM's Response ---\n",
    "        # Check if the LLM decided to call a tool\n",
    "        if 'tool_calls' in response['message'] and response['message']['tool_calls']:\n",
    "            print(\"\\n--- LLM decided to call a tool! ---\")\n",
    "            for tool_call in response['message']['tool_calls']:\n",
    "                tool_name = tool_call['function']['name']\n",
    "                tool_args = tool_call['function']['arguments']\n",
    "                tool_id = tool_call['id'] # Each tool call has a unique ID\n",
    "\n",
    "                print(f\"  Tool Name: {tool_name}\")\n",
    "                print(f\"  Tool Arguments: {json.dumps(tool_args, indent=2)}\")\n",
    "\n",
    "                # Here, you would execute the actual Python function based on tool_name\n",
    "                if tool_name == \"get_current_weather\":\n",
    "                    # Call your Python function with the arguments extracted by the LLM\n",
    "                    weather_result = get_current_weather(**tool_args)\n",
    "                    print(f\"\\n--- Executing tool '{tool_name}'... ---\")\n",
    "                    print(f\"  Tool Execution Result: {weather_result}\")\n",
    "\n",
    "                    # Optionally: Send the tool's output back to the LLM for it to summarize or refine\n",
    "                    # This is how multi-turn reasoning with tools works.\n",
    "                    # messages.append({\"role\": \"tool\", \"content\": weather_result, \"tool_call_id\": tool_id})\n",
    "                    # final_response = ollama.chat(model=model_name, messages=messages)\n",
    "                    # print(f\"\\n--- LLM's final response after tool execution: ---\")\n",
    "                    # print(final_response['message']['content'])\n",
    "\n",
    "                else:\n",
    "                    print(f\"Error: Unknown tool '{tool_name}' requested by LLM.\")\n",
    "        else:\n",
    "            # LLM returned a direct text response (no tool call)\n",
    "            print(\"\\n--- LLM returned a direct text response (no tool call): ---\")\n",
    "            print(response['message']['content'])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(\"Please ensure your Ollama server is running and the model is downloaded (`ollama run llama3.2`).\")\n",
    "\n",
    "# --- Run the Example Queries ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Test case 1: User asks a question that requires the tool\n",
    "    chat_with_llm_with_tools(\"What's the weather like in Manchester?\", model_name=\"llama3\")\n",
    "\n",
    "    # Test case 2: User asks with a specific unit\n",
    "    chat_with_llm_with_tools(\"How many degrees Fahrenheit is it in London?\", model_name=\"llama3\")\n",
    "\n",
    "    # Test case 3: User asks about a location not handled by our dummy tool\n",
    "    chat_with_llm_with_tools(\"What's the temperature in Paris?\", model_name=\"llama3\")\n",
    "\n",
    "    # Test case 4: User asks a general question (should not call tool)\n",
    "    chat_with_llm_with_tools(\"Tell me a fun fact about giraffes.\", model_name=\"llama3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mapping_py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
